# 6주차



## 인공지능/빅데이터 응용 시스템 평가



### 6-1. Classifier 성능 평가

- 활동 인식기가 얼마나 잘 작동하는지 알아보자

  - 객관적인 기준 만들기
  - 계속해서 성능 개선시키기 위해 굉장히 중요하다.

- 두가지 중요한 포인트

  - 트레이닝 데이터와 테스트 데이터를 어떻게 구별할지

  - 어떤 평가 지표를 가지고 classifier의 성능을 평가할지

    

- 트레이닝 데이터와 테스트 데이터를 어떻게 구별할지

  - 전체 데이터를 가지고 모델을 트레이닝
  - 이 데이터들을 그대로 테스트해본다
  - 이 방법은 문제가 있다.
  - 왜냐하면 이 모델을 트레이닝 한 상황과 미래의 상황은 굉장히 다른 양상일 수 있기 때문이다.
  - 오버피팅 - 내가 본 데이터에 대해서만 특화된다.(문제점)
  - 그래서 트레이닝 데이터를 테스트 과정에서 다시 활용되지 않도록 주의해야 한다.
  - Cross Validation
    - 예를 들어, 100개의 데이터가 있다고 하면, 100개 중에 한 20개 정도를 빼서 놔두고 나머지 80개 정도만 이용해서 classifier를 트레이닝한다.
    - 그리고 이렇게 트레이닝 된 모델을 활용하지 않았던 20개의 데이터에 대해서 평가를 해보는 과정 진행
    - 한마디로 테스트셋과 트레이닝셋을 나눠서 평가에 활용하는 것!
  - Holdout Method
    - 예를들어 3분의 2정도는 트레이닝을 위해 쓰고 3분의 1정도는 테스트를 위해 쓰겠다 이렇게 나누는 것
    - 주의해야 할 점은 특정 데이터가 한쪽에만 집중해서 들어가지 않도록 해야하는 것이다.
    - 여러번 수행해봐야 한다.
      - 랜덤하게 하더라도 어떤 데이터는 많이 사용되고, 어떤 데이터는 적게 활용될 수 있는 문제가 있다.
      - 이를 방지하기 위한 방법은 N-Fold Cross-Validation
        - 데이터를 N개의 그룹으로 나누고 N개의 그룹 중 N-1개의 그룹은 트레이닝 데이터로 활용을 하게 되고, 마지막 N번째 그룹 혹은 특정 N번 그룹의 데이터는 테스트를 위해서 활용하게 된다. 
        - N은 10 정도로 잡아서 하는게 성능을 잘 반영한다는 논문이 많다.
  - Leave-One-Out Cross-Validation
    - N-Fold Cross-Validation의 특수한 형태
    - 예를 들어 100개의 데이터가 있다고 하면 그룹의 사이즈가 1이 되는 것이다. 그래서 99개의 데이터를 트레이닝 데이터로 활용하고, 하나의 데이터만 테스트 데이터로 활용하는 것이다.
    - N개의 데이터가 있을 때 N개의 그룹이 생성되게 되고 이걸 N라운드 반복해야하는 그런 특성이 있다.
    - 단점은 그룹을 굉장히 많이 나누고 여러번 반복해야된다는 점이다.
  - Bootstrap Method
    - 그룹을 사전에 나누지 않고 그냥 랜덤하게 데이터를 뽑아서 데이터 트레이닝 셋의 칸을 메꾼다. 하나의 데이터가 여러번 뽑혀서 트레이닝 데이터에 여러번 활용될 수 있다.
    - 그래서 특정 방향으로 편향되지 않도록 하기 위해 보정해주는 과정을 거친다.
    - 테스트 인스턴스에 대해서는 63.2% 정도만 반영하고 트레이닝 인스턴스에 대해서는 36.8% 에 대해서만 반영해준다.
  - Rule of Thumb
    - 데이터가 충분히 많고 큰 경우에는 Holdout Method 활용
    - 중간 정도면 Cross Validation 활용
    - 작으면 Leave-One-Out 이나 Bootstrap Method 활용

- 모델의 성능을 보여주는 흔히 사용하는 매트릭 중에 하나가 Confusion Matrix
  - 데이터가 어느 곳에 해당하는지 알게되고

  - 그 다음에 classifier를 돌렸을때 결과가 각 케이스에 대해서 어떻게 나오는지를 알게 된다.

  - 그래서 Known class ground truth 와 predicted class classifier가 예측한 클래스들 사이에 얼마나 들어맞는지를 보여주는 것이 Confusion Matrix이다.

  - Binary Classifier(두 개의 상황)

  - |      | A    | B    |
    | ---- | ---- | ---- |
    | A    | TP   | FN   |
    | B    | FP   | TN   |

    ![캡처](md-images/%EC%BA%A1%EC%B2%98-1634009326312.PNG)

- 또 다른 matrix 중에 하나가 F-measure

![캡처](md-images/%EC%BA%A1%EC%B2%98-1634011273367.PNG)

- 또 다른 matrix 중에 흔히 사용하는 것은 ROC

![캡처](md-images/%EC%BA%A1%EC%B2%98-1634011333079.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1634011360108.PNG)

- ​	어떤 classifier 가 다른 classifier보다 dominate한다 즉 좋다라고 얘기할 때는 TP가 높은 동시에 FP가 낮아야 한다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1634011503035.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1634011602128.PNG)

### 6-2. 애플리케이션 사용성 평가

- 정확성이 높다고 해서 사용성이 높은것이냐?? - 아니다.
- 사용자 - 애플리케이션 사용자
  - 디자이너도 아니고 developer도 아니다.
- 사용성이 높다
  - Quality
  - Learnability
  - Efficiency
  - 등등
- Usability 는 중요하다.
- Usable한 애플리케이션을 만드는 것은 어렵다
  - 왜냐하면 복잡하게 만드는 것은 쉬운데 단순하게 만드는 것은 어렵다.
  - 창의적인 사고가 필요하다.
  - 한가지 정답이 있는 것이 아니다.
- Application Design Process
  - Establish Requirements -> (Re)Design -> Prototype/Implement -> Evaluate -> Establish Requirements  ~~~~ 계속 반복
- Prototypes
  - 스토리보드, 페이퍼 등등
  - 사용자들은 자기가 뭐를 원하는지 잘 표현하지 못한다.
  - 그래서 중간중간 확인용
  - Prototypes을 확인하는 방법
    - Cognitive Walkthrough
      - 자기 스스로 질문을 던져가면서 판단해보는 것
    - Heuristic Evaluation
      - 소수의 전문가를 통해서 피드백 받는 것
      - 저비용
      - 거의 회사의 동료들 통해서 피드백 받는다.
      - 80%의 사용성 문제는 5명의 사용자만 가지고 테스트 해도 확인할 수 있다.
        - 과정
        - 1. Briefing
          2. Evaluation Period
             1. 각각 평가(independently)
          3. Debriefing
    - User Tests
      - 실제 사용자들을 데려다가 테스트해보는 것
      - 두가지 환경
        - 개발자가 세팅해놓은 환경에서 테스트
          - 예상치 못한 변수가 실제 상황 반영 못할 수 있음
        - natural한 상황에서 테스트(Field Studies)
          - 얻고자 하는 것을 못 얻을 수 있다.

