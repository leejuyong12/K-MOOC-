# 5주차



## 위치 탐지 기술 및 응용



### 5-1. 딥러닝의 활용 개요

- 자율주행, 알파고, face recognition
- 인공지능 application이 흔히 활용하는 pipe line
- 딥러닝은 센싱을 바로 해서 피쳐를 뽑는 과정이라든지 전처리 과정을 최소화하고 전처리 과정과 classification 과정을 합쳐서 추론하게 되고 그 추론 결과를 바탕으로 application을 만든다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633485553303.PNG)
- 워킹, 조깅, 시팅, 스탠딩과 같이 일상에서 사람들이 하는 활동을 구별하기 위한 classifier를 어떻게 디자인할까에 대해 이전에 공부했었다.
  - 첫번째 방법은 휴리스틱을 활용하는 것이다.
  - ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633486198107.PNG)
  - 엑셀러레이션 값의 변화에 따라 액티비티를 구별하게 된다.
  - 그 안에서 FFT를 돌려서 시그널의 프리퀀시에 따라서 워킹과 조깅을 구별해준다든지
  - 그리고 중력가속도가 어떤 방향으로 센싱되는지에 따라서 스탠딩과 시팅을 구별해준다든지 하는 이런 휴리스틱 기법이 있다.
  - 이 방법에는 문제점이 있다.
    - 이 방법은 사람이 시그널을 직접 보고 유의미한 피쳐와 Threshold를 직접 결정해줘야 된다는 문제가 있고 다양하게 변화하는 상황에 대처하기가 어렵다는 문제가 있다.
- 위와 같은 문제를 극복하기 위해 Decision Tree가 있다.
  - Decision Tree는 시그널에서 발생하는 여러 액티비티를 구별하기 위한 유의미한 피쳐들과 Threshold 값을 자동으로 데이터에 기반해가지고 구별해주는 방법이다.

- 이 외에도 다양한 머신러닝 테크닉들이 있다.
  - Naive Bayes classifier
  - Random Forest
  - Support vector machine
  - knn algorithm
  - Linear regression
- 이런 머신러닝 테크닉들은 공통적인 플로우를 거치게 된다.
  - 트레이닝 데이터가 있고 그 데이터로부터 피쳐를 뽑아서 어떤 피쳐가 유용한지 그리고 어떤 게 구별의 기준이 되는지를 학습을 통해서 모델을 만들게 된다.
  - 그 모델에다가 새로운 데이터가 들어왔을 때 이 모델을 기반으로 새로운 데이터가 어떤 카테고리에 해당하는지를 추론을 하게 되는 공통적인 과정을 거치게 된다.

- 머신러닝 테크닉의 한계
  - 피쳐를 개발자들이 직접 슈퍼셋을 뽑아줘야한다.
  - 인풋과 아웃풋의 매핑이 런리니어하고 굉장히 복잡한 경우에는 단순한 종류의 기존의 머신러닝 테크닉들이 동작을 하지 않게 된다.
- 이러한 한계를 극복하기 위해 딥러닝 기술들이 출현을 하고 있다.
  - 딥러닝 기술은 흔히 피쳐를 뽑을 필요가 없다.
  - raw 인풋 데이터를 가지고 딥러닝 모델을 직접 학습하게 된다.
  - 새로운 raw 데이터가 왔을때 딥러닝 모델을 활용해서 그것이 어떤 카테고리에 해당하는지를 직접 탐지할 수 있는 그런 장점이 있다.
  - Activity Recognition같은 경우에는 딥러닝을 적용해서 정확도를 높이기 위한 기술들이 많이 출현을 하고 있다.
  - ex) 시그널을 시간축상으로 바로 프리젠트를 하게 된다. 시간축상에서 프리젠트를 하게 되면 시그널은 그냥 하나의 이미지로 간주를 할 수 있게 된다. 비쥬얼한 인포메이션을 잘 파악하기 위해 convolutional neural network를 사용해서 하나의 이미지로 딥러닝 모델에 주게 되고 이 이미지를 바탕으로 딥러닝 모델을 트레이닝 해서 액티비티를 탐지하게 되면 더 높은 정확성을 가지는 Activity Recognition Model을 만들수 있다는 연구결과들이 나오고 있다.
  - ex) speech recognition에서도 딥러닝 기술을 많이 활용하고 있다. 음성데이터를 frequency domain 상에서 spectogram을 그리게 되고, 이것이 하나의 visual image가 된다. 이 visual image를 바로 딥러닝 모델을 활용해가지고 이것은 누구의 말에 해당하는 거다, 어떤 사람이 얘기하는 거다 혹은 어떤 단어에 해당하는 거다를 바로 탐지하게 되는 그런 기술들이 출현을 하고 있다.
- 마지막으로 딥러닝이 가지는 기존의 머신러닝 기술 대비 중요한 특성
  - 딥러닝 알고리즘의 경우에는 모델이 커지고 데이터가 많아질수록 정확성을 점차 더 높일 수 있다는 특성이 있다.



- 요약
  - 딥러닝은 피쳐를 뽑지 않아도 된다.
  - 인풋과 아웃풋 사이에 굉장히 더 복잡한 모델링을 할 수 있다.
  - 데이터와 모델 사이즈가 늘어남에 따라 더욱 더 높은 정확성을 확보할 수 있다.

### 5-2. 합성곱 신경망

- CNN(Convolutional Neural Network)
  - 피트 포워드 네트워크의 일종
  - 이미지를 인식하기 위하 task를 잘한다.
  - 이미지 안에서 visual pattern을 분석해서 어떤 이미지인지 탐지
  - Input layer, Hidden layer, Output layer로 구성됨
  - 최종적으로 fully-connected layer를 거쳐서 최종적인 추론을 하게 된다.

- convolutional layer의 핵심이 되는 요소는 convolutional filter이다.
- 트레이닝을 하면 필터는 자동으로 생겨난다.
- 두 벡터 간에 닷 프로덕트 = 합성곱

- 필터를 적용해서 convolution operation 한 데에서 나온 결과를 피쳐 맵이라고 부른다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633505225018.PNG)
- 흔히 컬러 이미지는 RGB 값이 있는 three 채널의 이미지이기 때문에 이 과정을 세번 반복해줘야 한다.
- CNN이 가지는 큰 장점 중에 하나가 보통 피드 포워드 뉴럴 네트워크 같은 경우에는 왼쪽에 있는 모든 노드들과 각 오른쪽에 있는 노드들의 모든 노드가 연결되어야 하는 그런 특성을 가지고 있는데, CNN같은 경우에는 그중에서 굉장히 적은 수의 노드들간에만 이 엣지가 형성되게 되는 그런 특성을 가지고 있다.
- Convolution layer는 Convolution Filter를 이미지에 sub area에 계속해서 적용해나가면서 그 이미지에 어떤 특징이 될 만한 컴포넌트들이 있는지를 살펴보는 그런 레이어이다.
- 여러 번의 컨볼루션 레이어를 겪어나가면서 이미지를 계속해서 요약을 한다.
- Pooling layer
  - pooling - 피처 맵을 요약하는 과정
- Convolutional layer 하나와 Max pooling layer 하나를 거치고 나면 6x6 이미지가 2x2이미지로 요약이 된다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633511340160.PNG)

- Fully Connected Layer
  - 최종적으로 인식해야 될 대상을 classify하는 과정
  - 최종적으로 요약되어서 나온 피쳐값들 중에 어떤 부분 그리고 어떤 값들이 최종 결과에 어떻게 영향을 미치는지를 voting 하는 과정을 모델링
  - ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633511834269.PNG)

### 5-3. 모바일 딥러닝 소개

- 모바일 화면으로 AR어플리케이션을 활용해 어떤 상점들이 있는지 탐지하는 것들이 나오고 있다.

- 이것의 프로세스를 살펴보면

  - 이미지 프레임들을 캡쳐해서 이것을 클라우드나 서버로 보내게 되고, 이 서버에서 각종 deep neural network model 을 돌려서 인식하고자 하는 대상을 인식하게 되는 그런 flow를 지니게 된다.

- 이 클라우드 기반의 처리에는 두가지 문제점이 있다.

  - 하나는 private한 영상 데이터를 클라우드로 계속해서 전송해야 하는 그런 문제가 있다.
  - 또 다른 문제는 영상 데이터가 용량이 크다는 문제가 있다.

- 그래서 클라우드로 전송하지 않고 초소형 기기, 모바일기기, 웨어러블 기기 같은 소형기기에서도 딥러닝 모델을 수행해서 인식을 바로 할 수 있지 않을까 하는 연구가 이루어지고 있다. 이것은 쉬운일이 아니다.

  - 어려운점

    - 첫번째
    - neural network processing 같은 경우에는 highly paral processing이 필요하기 때문에 GPU를 많이 활용하게 된다.
    - 모바일 GPU는 이제 막 모바일 기기들에 도입이 되고 있고, 데스크탑 GPU에 비해서도 상당히 성능이 약하게 된다.
    - 모바일 GPU에서 딥러닝 기반의 CNN을 execution 했을때와 데스크탑 GPU를 통해서 이것을 execution했을 때 그 성능 차이가, execution 시간 차이가 상당히 나타나게 된다.
    - 두번째

    - 모바일 GPU가 데스크탑 GPU에 비해서 성능이 굉장히 약할 뿐 아니라 architectural 특징도 다르게 가지게 된다.
    - 데스크탑 GPU의 경우에는 GPU 안에 GPU 메모리를 따로 가지게 되고 거기에 한 번 데이터를 올리게 되면 GPU는 시스템의 다른 부분, CPU라든지 다른 프로세서와는 상관없이 독립적인 프로세싱을 진행할 수 있게 된다.
    - 모바일 GPU의 경우에는 GPU와 CPU 그리고 다른 프로세스도 메모리를 세워하는 구조를 가지고 있기 때문에 메모리 bandwidth가 작고 더군다나 그 메모리를 쉐어해서 써야하기 때문에 성능은 더 느려질 수 밖에 없는 그런 특징을 갖게 된다.

- CNN을 모바일에서 어떻게 하면 더 빠르게 수행할 수 있을까?

  - 어떤 layer가 병목(bottleneck)인지 살펴보자

  - Convolutional layer가 다른 레이어에 비해 수행 시간이 현저하게 크다.(이걸 개선해야한다.)

  - ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633529223389.PNG)

    - GEMM 라이브러리 활용

    - Convolutional operation 은 dot product이다.

    - dot product를 matrix multiplication으로 변환을 해서 수행하자.

    - 그러면 이미지에 대해서 수행해야 되는 모든 convolution 연산이 한번에 수행된다라는 걸 알 수 있다.

    - 이 방법을 사람들이 많이 활용하는 이유는 matrix multiplication에 대해서 굉장히 많은 optimization이 이루어져 있고 GEMM이 굉장히 빠르게 돌아가기 때문이다.

    - 하지만 이 방법은 데스크탑 GPU에서는 큰 문제가 되지 않았고 굉장한 성능의 향상을 가져왔는데 모바일 상황에서는 이 방법이 굳이 좋은 방법이 아니라는 것을 알 수 있다.

      - why?

      - GEMM을 활용해서 Convolutional Operation을 수행하려면 input image를 matrix로 변환하는 그런 과정이 필요하다.

      - 이 변환의 과정이라는 것은 메모리를 읽어서 리드를 하고 다시 쓰는 과정이 필요하다.

      - 모바일 GPU는 메모리 연산이 굉장히 느리고 또 다른 프로세스들과 쉐어링을 하기 때문에 input image를 GEMM하기 위한 matrix로 conversion하는 과정 자체가 overhead된다.

      - 그리고 다른 문제로는

      - memory operation이 bottleneck이 되는 상황에서 추가적인 카피가 일어나야 하고 리드가 일어난다는 것은 실행 속도에 영향을 미칠 수 밖에 없게 된다.

      - 그리고 또 다른 문제는

      - 모바일 GPU에서 돌아가는 GEMM 구현은 사실상 현재는 어렵다

        

      - 그 다음 큰 문제중에 하나는 모바일에서 응용을 살펴보면 흔히 continuous vision application들이 많다. 연속적인 이미지 프레임에 대해서 인식을 이어나가야 한다는 문제가 있다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1633529793456.PNG)

- 내가 보는 장면이 급격하게 변하는 경우가 드물다. 급격하게 변하지 않는 scene들에 대해서 계속해서 deep neural network 연산을 수행하게 되면 연산에 굉장히 많은 redundancy(불필요함)가 발생을 하게 된다.





- 위에서 언급한 문제를 해결하는 기술들이 많이 출현을 하고 있다.
  - 첫번째로 Convolutional Operation을 GEMM으로 변환하지 않고 원래 하려던 방식대로 direct Convolution을 수행하는 그런 방식으로 수행하고자 하는 연구들이 많다.
  - 또, Convolution Operation을 FFT를 기반으로 approximation해서 가속화하기 위한 그런 기술들도 있고
  - 모바일 GPU에 굉장히 작은 local memory가 있다. 그 local memory를 잘 활용해서 unvolution 연산을 빠르게 하기 위한 그런 기술들도 출현하고 있다.
  - 또 다른 방식으로는 숫자의 연산, 수의 표현의 정확도를 32비트에서 16비트로 hardfloating point를 활용해서 Convolutional operation의 연산을 가속화히기위한 그런 기술들도 제안이 되고 있다.
  - 또 다른 방식으로는 여러 이미지 프레임간의 연산의 redundancy를 해결하기 위한 기술도 소개되고 있다. 예를 들어 2개의 consecutive image가 있다고 생각을 했을 때 그 두개의 이미지 사이의 similarity를 이미지의 sub area를 분석해서 비슷한 이미지의 sub 영역이 있다고 하면 convolutional 연산을 굳이 다시 수행을 하지 않고 기존의 convolutional 연산을 재활용하게 되고, 만약에 이 두개의 이미지 프레임 사이의 sub 영역이 굉장히 다르게 나타난다라고 하면 다시 convolutional operation 을 수행하는 그런 기술들도 제안이 되고 있다.
  - ![캡처](md-images/%EC%BA%A1%EC%B2%98-1633530359097.PNG)
  - 또 다른 기술로는 matrix decomposition 기술도 제안이 되고 있다. 이것은 큰 matrix 연산을 하나의 큰 matrix 연산으로 하는게 아니라 여러 개의 작은 matrix연산으로 나눠서 수행하게 되면 물론 정확성의 저하가 있을 수 있지만 연산을 더 빠른 속도로 가속화할수 있다는 그런 것을 알게 된다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1633530510449.PNG)

요약

딥러닝을 IOT나 모바일 환경의 인공지능 응용에 활용하기 위해서는 정확성 문제 뿐만 아니라 latency라든지 execution 의 속도라든지 파워 소모라든지 이런 시스템 이슈들에 대해서도 관심을 갖고 최적화를 해주는 과정이 꼭 필요하다.
