# 4주차



## 강화학습 소개



### 4-1. 강화학습 소개

- 스스로 할 수 있게 한 다음에 행동을 잘했으면 보상, 못했으면 벌을 준다.
- 보상함수를 통해 스스로 학습
- feedback이 한참 후에 주어질 수도 있다.
- non i.i.d - non independent identical distribute
  - 학습 데이터가 서로 다 독립 사건이 아니다.
- 에이전트는 주변 환경과 상호작용하면서 학습해나간다.
- reward를 통해서 컨트롤, 어떻게 정의하느냐가 중요하다.
- Agent
  - Policy - 어떤 상태에서 어떤 행동을 할지 정해주는 함수
    - Deterministic policy
      - 어떤 상태에서 어떤 액션을 할지 하나하나 정의해놓음
    - Stochastic policy
      - 어떤 state에서 어떤 액션을 취할지 확률분포로 주어지는 것
      - 어떤 액션을 취할 확률이 높긴 하지만 항상 그 액션을 취하는 것은 아니다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635569469473.PNG)
  - Value function - 어떤 state에 갔을 때 그 state가 얼마나 좋은지, 내가 reward를 계속 maximizing하는 방향으로 행동을 할텐데, 내가 어떤 state에서 어떤 state로 갈 때 그 state가 얼마나 좋은 지 나타내는 함수를 value function이라고 한다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635569501798.PNG)
  - ​	아래첨자 파이의 의미는 파이라는 policy를 따라갈 때의 value function이 이렇게 주어진다. 
    - 미래에 대한 기댓값
  - Model - 에이전트가 환경을 어떻게 표현하고 있는지

- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635569521050.PNG)

- P는 S라는 state에 있을 때 a라는 액션을 취하면 S`라는 state로 갈 확률을 나타낸다.
- R은 S라는 state에 있을 때 a라는 액션을 취하면 그때 얻는 reward



- Exploration and Exploitation
  - 많은 경우에 경험이 쌓이면 쌓일수록 경험상 '이런 액션을 취했을 때 reward가 많았으니까 이런 reward를 취해야지' 이런식으로 Exploitation, 알고 있는 지식을 활용하는 경우이다. 하지만 그것만 활용해서는 안된다.
  - 왜냐하면 처음에 두번째로 좋은 액션을 취해서 reward 1을 받았는데, 원래 처음에 제일 좋은 액션을 취했으면 reward를 10을 받을 수 있었는데 미처 그걸 안했다. 그럴 경우 제일 좋은 액션을 안하는 현상이 벌어질 수 도 있기 떄문이다.
  - 그래서 많은 경우에는 Exploitation, 내가 알고 있는 정보를 활용해서 reward를 maximizing하는 액션을 하는 것도 중요하지만, 일종의 random action을 취해서 '내가 이런 상황에서 어떤 액션을 하면 이런 reward를 받는구나' 이런 경험도 쌓이게 된다.
  - 그래서 Exploration 과 Exploitation 사이에는 trade-off가 발생한다.
  - Exploration
    - 새로운 것을 시도
  - Exploitation
    - 현재까지 알고 있는 것을 바탕으로 제일 좋다고 생각하는 것 활용
- Prediction and Control
  - Prediction
    - Policy가 주어졌을 때 value function을 구하는 것, 그 작업을 Prediction이라고 한다.
  - Control
    - 문제가 주어졌을 때 최적의 Policy, 그러니까 reward를 최대한으로 할 수 있는 policy를 찾는 과정

### 4-2. 마르코프 과정

- Markov Decision Processes(MDP)
  - S - state의 집합
  - P - state transition function(state를 받아서 또 다른 state로 맵핑해주는 함수)
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635577266916.PNG)

- Markov property
  - 현재 state까지 오는데 어떤 히스토리를 겪었든지 간에 현재 state만 알면 다음 state가 어디로 갈지에 대한 확률분포가 정해진다.(독립)
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635577497656.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635577586570.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635577698184.PNG)

- V(S1)이 무한대가 되는 문제가 생길 수 있다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635577805562.PNG)

