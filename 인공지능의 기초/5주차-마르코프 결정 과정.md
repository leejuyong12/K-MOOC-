# 5주차



## 마르코프 결정 과정



### 5-1. 마르코프 결정 과정

###  ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582178814.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582298026.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582354852.PNG)

- Bellman Expectation Equation for MDPs.
  - 우리가 정의한 value function에 대해서 현재 state의 value function과 그 다음 state의 value function의 상관관계를 정의한 것이다.
  - 어떤 state에서 Reward를 받고 다음 state로 갔을때 현재 state에서의 value function과 Reward를 받고 다음 state로 갔을때 그다음 state에서의 value function의 관계를 배운다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582630966.PNG)

- 위의 첫번째 수식과 마지막 수식을 비교한다면, value function이라는 건 현재 step 의 return의 Expected value인데 그것은 Immediate reward에 그 다음 state의 value function을 감마 만큼 discount한 것과의 합으로 표현할 수가 있다. 
- value function은 두 부분으로 나눠지는데, Immediate reward, 즉 당장 받을 수 있는 Reward에 대한 Expectation 그리고 다음 state로 갔을 때의 value function에 대한 Expectation 두개로 표현된다.
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582875439.PNG)

- 두 공식은 마코프에서 가장 중요
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635582959238.PNG)

- state에서 action으로 가는 과정
- action에서 state로 가는 과정
- ![캡처](md-images/%EC%BA%A1%EC%B2%98-1635583597844.PNG)

- Q는 Q로 정의
- V는 V로 정의

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635583822766.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635583993533.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635584060066.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635584228934.PNG)

### 5-2. 동적 프로그래밍을 통한 마르코프 결정 과정

- Dynamic Programming
  - 주어진 문제가 있다고 하면 그 주어진 문제를 여러 개의 sub-problem, 더 작은 문제로 나눌 수가 있다.
  - 그리고 중요한 건 그 나눠진 작은 문제에 대해서 솔루션을 구하면, 그 솔루션이라는 것이 원래 복잡한 문제를 푸는 솔루션을 구하는 데 사용이 된다는 것이다.
  - 그러니까 원래 문제를 계속 작은 문제로 나눠가고, 충분히 작아진 문제에 대해서 솔루션을 구해서 그 솔루션을 활용해 그것보다 더 높은, 더 어려운 문제에 대한 솔루션을 구하고 구하는 이런 과정이 가능한 경우에 Dynamic Programming을 적용하게 된다.
- MDP는 이 두가지 특성을 다 만족하기 때문에 DP를 활용해서 Bellman Expectation Equation을 풀 수가 있다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635585996345.PNG)

- optimal policy라는 건 optimal value function을 구하면 쉽게 구할 수가 있다.
- 그래서 많은 경우에 Control 문제라고 하면 optimal policy를 구하는 문제이긴 하지만, 그냥 optimal value function을 구하는 문제를 Control이라고 부르기도 한다.
- 그리고 MDP가 주어졌다는 말은 S도 알고, A도 알고, P, R도 알고, 감마도 알고 있다. 이게 다 주어졌다고 가정을 한 상태이다.

- 그래서 이 5개의 component가 주어졌을 때, policy가 추가적으로 주어지면 value function을 구하는 문제, 그것이 Prediction이다.
- MPD만 주어졌을 때 optimal policy를 구하는 과정이 Control이다.

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635586224431.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635586778576.PNG)

![캡처](md-images/%EC%BA%A1%EC%B2%98-1635586981991.PNG)

- V가 Complexity가 더 적다.
- Q는 각 s에 대해서, 각 a에 대해서 다 정의를 해줘야 한다.
